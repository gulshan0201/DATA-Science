{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKHWXjMZU3tQsLw6uO3sUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulshan0201/DATA-Science/blob/main/ML_LAB_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "# 1) Generate complex dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=5000, n_features=200, n_informative=30, n_redundant=30, n_repeated=10,\n",
        "    n_classes=3, weights=[0.6,0.3,0.1], flip_y=0.02, random_state=42\n",
        ")\n",
        "\n",
        "# Inject heterogeneous scales and outliers\n",
        "X[:, 0:50] *= 0.1\n",
        "X[:, 50:100] *= 10\n",
        "X[:, 100:150] *= 100\n",
        "rng = np.random.RandomState(42)\n",
        "outlier_cols = slice(150, 170)\n",
        "X[:, outlier_cols] += rng.standard_t(df=2, size=X[:, outlier_cols].shape) * 20\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n",
        "\n",
        "# 2) Define scalers and models\n",
        "scalers = {\n",
        "    \"None\": None,\n",
        "    \"StandardScaler\": StandardScaler(),\n",
        "    \"MinMaxScaler\": MinMaxScaler(),\n",
        "    \"RobustScaler\": RobustScaler()\n",
        "}\n",
        "\n",
        "models = {\n",
        "    \"KNN(k=11)\": KNeighborsClassifier(n_neighbors=11),\n",
        "    \"SVM(RBF)\": SVC(kernel='rbf', C=10, gamma='scale'),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=3000, multi_class='multinomial'),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "scoring = {'acc': make_scorer(accuracy_score), 'f1': make_scorer(f1_score, average='macro')}\n",
        "\n",
        "def evaluate_combo(scaler_name, scaler, model_name, model):\n",
        "    steps = []\n",
        "    if scaler is not None:\n",
        "        steps.append(('scaler', scaler))\n",
        "    steps.append(('model', model))\n",
        "    pipe = Pipeline(steps)\n",
        "    cvres = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    return {\n",
        "        'Scaler': scaler_name,\n",
        "        'Model': model_name,\n",
        "        'CV_Acc_Mean': np.mean(cvres['test_acc']),\n",
        "        'CV_F1_Mean': np.mean(cvres['test_f1'])\n",
        "    }\n",
        "\n",
        "results = []\n",
        "for s_name, scaler in scalers.items():\n",
        "    for m_name, model in models.items():\n",
        "        results.append(evaluate_combo(s_name, scaler, m_name, model))\n",
        "\n",
        "# Print sorted by Macro-F1\n",
        "results_sorted = sorted(results, key=lambda d: d['CV_F1_Mean'], reverse=True)\n",
        "for r in results_sorted:\n",
        "    print(r)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzcRuD6ZXzkb",
        "outputId": "a460959c-b14c-4807-e25b-a4a75924ee07"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Scaler': 'MinMaxScaler', 'Model': 'SVM(RBF)', 'CV_Acc_Mean': np.float64(0.844), 'CV_F1_Mean': np.float64(0.7562210596836134)}\n",
            "{'Scaler': 'StandardScaler', 'Model': 'SVM(RBF)', 'CV_Acc_Mean': np.float64(0.8394999999999999), 'CV_F1_Mean': np.float64(0.7423688941292339)}\n",
            "{'Scaler': 'RobustScaler', 'Model': 'SVM(RBF)', 'CV_Acc_Mean': np.float64(0.8089999999999999), 'CV_F1_Mean': np.float64(0.7085499342651727)}\n",
            "{'Scaler': 'None', 'Model': 'SVM(RBF)', 'CV_Acc_Mean': np.float64(0.7835), 'CV_F1_Mean': np.float64(0.6950960269368792)}\n",
            "{'Scaler': 'None', 'Model': 'LogisticRegression', 'CV_Acc_Mean': np.float64(0.7384999999999999), 'CV_F1_Mean': np.float64(0.6298296903536219)}\n",
            "{'Scaler': 'StandardScaler', 'Model': 'LogisticRegression', 'CV_Acc_Mean': np.float64(0.73125), 'CV_F1_Mean': np.float64(0.616696113507028)}\n",
            "{'Scaler': 'RobustScaler', 'Model': 'LogisticRegression', 'CV_Acc_Mean': np.float64(0.7305), 'CV_F1_Mean': np.float64(0.615353741718254)}\n",
            "{'Scaler': 'MinMaxScaler', 'Model': 'LogisticRegression', 'CV_Acc_Mean': np.float64(0.739), 'CV_F1_Mean': np.float64(0.6111660747565271)}\n",
            "{'Scaler': 'StandardScaler', 'Model': 'KNN(k=11)', 'CV_Acc_Mean': np.float64(0.73925), 'CV_F1_Mean': np.float64(0.5422717440170404)}\n",
            "{'Scaler': 'MinMaxScaler', 'Model': 'KNN(k=11)', 'CV_Acc_Mean': np.float64(0.74125), 'CV_F1_Mean': np.float64(0.5342496903236021)}\n",
            "{'Scaler': 'None', 'Model': 'DecisionTree', 'CV_Acc_Mean': np.float64(0.642), 'CV_F1_Mean': np.float64(0.5285569428142847)}\n",
            "{'Scaler': 'StandardScaler', 'Model': 'DecisionTree', 'CV_Acc_Mean': np.float64(0.642), 'CV_F1_Mean': np.float64(0.5285569428142847)}\n",
            "{'Scaler': 'MinMaxScaler', 'Model': 'DecisionTree', 'CV_Acc_Mean': np.float64(0.642), 'CV_F1_Mean': np.float64(0.5285569428142847)}\n",
            "{'Scaler': 'RobustScaler', 'Model': 'DecisionTree', 'CV_Acc_Mean': np.float64(0.642), 'CV_F1_Mean': np.float64(0.5285569428142847)}\n",
            "{'Scaler': 'None', 'Model': 'KNN(k=11)', 'CV_Acc_Mean': np.float64(0.67775), 'CV_F1_Mean': np.float64(0.47945920149778043)}\n",
            "{'Scaler': 'RobustScaler', 'Model': 'KNN(k=11)', 'CV_Acc_Mean': np.float64(0.69875), 'CV_F1_Mean': np.float64(0.4785341526423233)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
        "\n",
        "\n",
        "# 1) Load dataset\n",
        "file_path = \"steam_sales.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "7vG8yQYibJua"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all duplicate values\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "7DXGqjhObQGD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df[categorical_cols] = df[categorical_cols].fillna(df[categorical_cols].mode().iloc[0])"
      ],
      "metadata": {
        "id": "KBqX5_kgbuD6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean #Reviews column (remove commas and convert to int)\n",
        "df['#Reviews'] = df['#Reviews'].astype(str).str.replace(',', '').str.split('.').str[0].astype(int)"
      ],
      "metadata": {
        "id": "mIpyoUDKbx1x"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Define Features (X) and Target (y)\n",
        "y = df['Rating']\n",
        "\n",
        "# Select numeric features\n",
        "X = df[['#Reviews', 'Discount%', 'Price (€)', 'Original Price (€)', 'Windows', 'Linux', 'MacOS']]\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=0)\n"
      ],
      "metadata": {
        "id": "gYtPC_MmcYhR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Define scalers and models\n",
        "scalers = {\n",
        "    \"None\": None,\n",
        "    \"StandardScaler\": StandardScaler(),\n",
        "    \"MinMaxScaler\": MinMaxScaler(),\n",
        "    \"RobustScaler\": RobustScaler()\n",
        "}\n",
        "\n",
        "models = {\n",
        "    \"KNN(k=11)\": KNeighborsClassifier(n_neighbors=11),\n",
        "    \"SVM(RBF)\": SVC(kernel='rbf', C=10, gamma='scale'),\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=3000, multi_class='multinomial'),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(max_depth=None, random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "DcJD0A38cmTh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Cross-validation setup\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "scoring = {'acc': make_scorer(accuracy_score), 'f1': make_scorer(f1_score, average='macro')}\n",
        "\n",
        "def evaluate_combo(scaler_name, scaler, model_name, model):\n",
        "    steps = []\n",
        "    if scaler is not None:\n",
        "        steps.append(('scaler', scaler))\n",
        "    steps.append(('model', model))\n",
        "    pipe = Pipeline(steps)\n",
        "    cvres = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    return {\n",
        "        'Scaler': scaler_name,\n",
        "        'Model': model_name,\n",
        "        'CV_Acc_Mean': np.mean(cvres['test_acc']),\n",
        "        'CV_F1_Mean': np.mean(cvres['test_f1'])\n",
        "    }"
      ],
      "metadata": {
        "id": "2hnZhUSxcslq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Run experiments\n",
        "results = []\n",
        "for s_name, scaler in scalers.items():\n",
        "    for m_name, model in models.items():\n",
        "        results.append(evaluate_combo(s_name, scaler, m_name, model))\n",
        "\n",
        "# Sort by F1 score\n",
        "results_sorted = sorted(results, key=lambda d: d['CV_F1_Mean'], reverse=True)\n",
        "\n",
        "# Convert to DataFrame for better presentation\n",
        "results_df = pd.DataFrame(results_sorted)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP4PnVxNcyTT",
        "outputId": "5ff7ee5f-7354-4fb7-9919-33a58190eb55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Scaler               Model  CV_Acc_Mean  CV_F1_Mean\n",
            "0   StandardScaler        DecisionTree     0.549656    0.497867\n",
            "1             None        DecisionTree     0.548787    0.496900\n",
            "2     MinMaxScaler        DecisionTree     0.547917    0.496441\n",
            "3     RobustScaler        DecisionTree     0.547048    0.488312\n",
            "4             None           KNN(k=11)     0.532254    0.288264\n",
            "5     RobustScaler           KNN(k=11)     0.540042    0.224314\n",
            "6   StandardScaler            SVM(RBF)     0.559214    0.217865\n",
            "7     MinMaxScaler           KNN(k=11)     0.545278    0.203766\n",
            "8   StandardScaler           KNN(k=11)     0.539203    0.198396\n",
            "9     MinMaxScaler            SVM(RBF)     0.563596    0.196277\n",
            "10  StandardScaler  LogisticRegression     0.564469    0.193195\n",
            "11            None  LogisticRegression     0.564462    0.185597\n",
            "12    RobustScaler            SVM(RBF)     0.564458    0.157291\n",
            "13    RobustScaler  LogisticRegression     0.562726    0.131306\n",
            "14            None            SVM(RBF)     0.562723    0.126805\n",
            "15    MinMaxScaler  LogisticRegression     0.563592    0.124277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a perturbed copy of the dataset\n",
        "X_perturbed = X.copy()\n",
        "X_perturbed['#Reviews'] = X_perturbed['#Reviews'] * 1000\n",
        "X_perturbed['Discount%'] = X_perturbed['Discount%'] * 0.01\n",
        "\n",
        "# Train-test split for perturbed data\n",
        "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
        "    X_perturbed, y, stratify=y, test_size=0.2, random_state=0\n",
        ")\n",
        "\n",
        "# Evaluate perturbed dataset\n",
        "results_perturbed = []\n",
        "for s_name, scaler in scalers.items():\n",
        "    for m_name, model in models.items():\n",
        "        results_perturbed.append(evaluate_combo(s_name, scaler, m_name, model))\n",
        "\n",
        "results_perturbed_sorted = sorted(results_perturbed, key=lambda d: d['CV_F1_Mean'], reverse=True)\n",
        "results_perturbed_df = pd.DataFrame(results_perturbed_sorted)\n",
        "\n",
        "# Compare with original results\n",
        "comparison_df = results_df.merge(\n",
        "    results_perturbed_df,\n",
        "    on=['Scaler', 'Model'],\n",
        "    suffixes=('_Original', '_Perturbed')\n",
        ")\n",
        "\n",
        "# Calculate sensitivity (absolute drop in F1)\n",
        "comparison_df['F1_Drop'] = comparison_df['CV_F1_Mean_Original'] - comparison_df['CV_F1_Mean_Perturbed']\n",
        "\n",
        "# Sort by sensitivity\n",
        "sensitivity_df = comparison_df.sort_values(by='F1_Drop', ascending=False)\n",
        "\n",
        "print(sensitivity_df[['Scaler', 'Model', 'CV_F1_Mean_Original', 'CV_F1_Mean_Perturbed', 'F1_Drop']])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Drxc_gKdJxa",
        "outputId": "8fba9ea7-04a6-475d-c482-18a78b212de0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Scaler               Model  CV_F1_Mean_Original  \\\n",
            "0   StandardScaler        DecisionTree             0.497867   \n",
            "1             None        DecisionTree             0.496900   \n",
            "2     MinMaxScaler        DecisionTree             0.496441   \n",
            "3     RobustScaler        DecisionTree             0.488312   \n",
            "4             None           KNN(k=11)             0.288264   \n",
            "5     RobustScaler           KNN(k=11)             0.224314   \n",
            "6   StandardScaler            SVM(RBF)             0.217865   \n",
            "7     MinMaxScaler           KNN(k=11)             0.203766   \n",
            "8   StandardScaler           KNN(k=11)             0.198396   \n",
            "9     MinMaxScaler            SVM(RBF)             0.196277   \n",
            "10  StandardScaler  LogisticRegression             0.193195   \n",
            "11            None  LogisticRegression             0.185597   \n",
            "12    RobustScaler            SVM(RBF)             0.157291   \n",
            "13    RobustScaler  LogisticRegression             0.131306   \n",
            "14            None            SVM(RBF)             0.126805   \n",
            "15    MinMaxScaler  LogisticRegression             0.124277   \n",
            "\n",
            "    CV_F1_Mean_Perturbed  F1_Drop  \n",
            "0               0.497867      0.0  \n",
            "1               0.496900      0.0  \n",
            "2               0.496441      0.0  \n",
            "3               0.488312      0.0  \n",
            "4               0.288264      0.0  \n",
            "5               0.224314      0.0  \n",
            "6               0.217865      0.0  \n",
            "7               0.203766      0.0  \n",
            "8               0.198396      0.0  \n",
            "9               0.196277      0.0  \n",
            "10              0.193195      0.0  \n",
            "11              0.185597      0.0  \n",
            "12              0.157291      0.0  \n",
            "13              0.131306      0.0  \n",
            "14              0.126805      0.0  \n",
            "15              0.124277      0.0  \n"
          ]
        }
      ]
    }
  ]
}