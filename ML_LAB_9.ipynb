{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPGEem01hg4Hs2eT6d71/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulshan0201/DATA-Science/blob/main/ML_LAB_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and Import Libraries\n",
        "\n",
        "!pip install torch torchvision\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Step 2: Load Dataset\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "\n",
        "# Step 3: Data Parallelism Example\n",
        "\n",
        "# Split dataset\n",
        "train1, train2 = random_split(train_dataset, [30000, 30000])\n",
        "loader1 = DataLoader(train1, batch_size=64, shuffle=True)\n",
        "loader2 = DataLoader(train2, batch_size=64, shuffle=True)\n",
        "\n",
        "# Model\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model = Net()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Helper function to compute gradients on one subset\n",
        "def compute_gradients(data_loader):\n",
        "    model.zero_grad()\n",
        "    for data, target in data_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        break  # process only one batch per subset for simplicity\n",
        "    grads = [p.grad.clone() for p in model.parameters()]\n",
        "    return grads\n",
        "\n",
        "# Training loop with gradient averaging\n",
        "for epoch in range(2):\n",
        "    grads1 = compute_gradients(loader1)\n",
        "    grads2 = compute_gradients(loader2)\n",
        "\n",
        "    # Average gradients\n",
        "    with torch.no_grad():\n",
        "        for p, g1, g2 in zip(model.parameters(), grads1, grads2):\n",
        "            p.grad = (g1 + g2) / 2.0\n",
        "\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} completed with averaged gradients.\")\n",
        "\n",
        "# Step 4: Model Parallelism Example\n",
        "\n",
        "# Define devices\n",
        "device0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device1 = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else device0)\n",
        "\n",
        "# Split model across two devices\n",
        "class SplitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SplitNet, self).__init__()\n",
        "        self.part1 = nn.Linear(28*28, 256).to(device0)\n",
        "        self.part2 = nn.Linear(256, 10).to(device1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28).to(device0)\n",
        "        x = F.relu(self.part1(x))\n",
        "        x = x.to(device1)\n",
        "        return self.part2(x)\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = SplitNet()\n",
        "criterion = nn.CrossEntropyLoss().to(device1)\n",
        "optimizer = optim.Adam([\n",
        "    {'params': model.part1.parameters()},\n",
        "    {'params': model.part2.parameters()}\n",
        "], lr=0.001)\n",
        "\n",
        "# One-batch demonstration\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "data, target = next(iter(train_loader))\n",
        "data, target = data.to(device0), target.to(device1)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = model(data)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Model parallelism step completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TZKO4y7lkbo",
        "outputId": "7b796f2c-de6c-4189-c9a2-9151dc546810"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 126MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 13.8MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 106MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed with averaged gradients.\n",
            "Epoch 2 completed with averaged gradients.\n",
            "Model parallelism step completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1:"
      ],
      "metadata": {
        "id": "s31myHp9qV4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import numpy as np\n",
        "\n",
        "# --- Task 1: Load and Describe the New Dataset ---\n",
        "\n",
        "print(\"--- Loading and Describing ENB2012_data.csv ---\")\n",
        "file_name = 'ENB2012_data.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Describe the dataset\n",
        "print(f\"Number of training samples: {len(df)}\")\n",
        "features = df.columns.drop(['Y1', 'Y2'])\n",
        "targets = ['Y1', 'Y2']\n",
        "print(f\"Features ({len(features)}): {list(features)}\")\n",
        "print(f\"Target variables ({len(targets)}): {list(targets)}\")\n",
        "print(\"This is a regression task, not a classification task.\")\n",
        "print(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "# Prepare data for PyTorch\n",
        "X = torch.tensor(df[features].values.astype(np.float32))\n",
        "y = torch.tensor(df[targets].values.astype(np.float32))\n",
        "full_dataset = TensorDataset(X, y) # Uses all 768 samples\n",
        "\n",
        "\n",
        "# --- Task 2: Adapt and Run Step 3 (Data Parallelism) ---\n",
        "\n",
        "print(\"--- Running Adapted Data Parallelism Example (Step 3) ---\")\n",
        "# Split dataset (768 samples -> 384 + 384)\n",
        "train1, train2 = random_split(full_dataset, [384, 384])\n",
        "loader1 = DataLoader(train1, batch_size=64, shuffle=True)\n",
        "loader2 = DataLoader(train2, batch_size=64, shuffle=True)\n",
        "\n",
        "\n",
        "# Model - ADAPTED for 8 inputs and 2 outputs\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 8 input features (X1-X8)\n",
        "        self.fc1 = nn.Linear(8, 128)\n",
        "        # 2 output variables (Y1, Y2)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # No .view() needed as data is already 1D\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "model_dp = Net()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "model_dp = model_dp.to(device)\n",
        "optimizer_dp = optim.Adam(model_dp.parameters(), lr=0.001)\n",
        "# Criterion - ADAPTED for regression\n",
        "criterion_dp = nn.MSELoss()\n",
        "\n",
        "# Helper function to compute gradients on one subset\n",
        "def compute_gradients(data_loader, model, criterion):\n",
        "    model.zero_grad()\n",
        "    try:\n",
        "        data, target = next(iter(data_loader))\n",
        "    except StopIteration:\n",
        "        print(\"Data loader is empty.\")\n",
        "        return None\n",
        "\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    grads = [p.grad.clone() for p in model.parameters() if p.grad is not None]\n",
        "    return grads\n",
        "\n",
        "# Training loop with gradient averaging\n",
        "for epoch in range(2):\n",
        "    grads1 = compute_gradients(loader1, model_dp, criterion_dp)\n",
        "    grads2 = compute_gradients(loader2, model_dp, criterion_dp)\n",
        "\n",
        "    if grads1 is None or grads2 is None:\n",
        "        print(f\"Epoch {epoch+1} skipped due to empty data loader.\")\n",
        "        continue\n",
        "\n",
        "    valid_params = [p for p in model_dp.parameters() if p.grad is not None]\n",
        "\n",
        "    if not grads1 or not grads2 or len(grads1) != len(valid_params) or len(grads2) != len(valid_params):\n",
        "        print(f\"Epoch {epoch+1} skipped due to gradient mismatch.\")\n",
        "        continue\n",
        "\n",
        "    # Average gradients\n",
        "    with torch.no_grad():\n",
        "        for p, g1, g2 in zip(valid_params, grads1, grads2):\n",
        "            p.grad = (g1 + g2) / 2.0\n",
        "\n",
        "    optimizer_dp.step()\n",
        "    print(f\"Epoch {epoch+1} completed with averaged gradients.\")\n",
        "\n",
        "print(\"-\" * 30 + \"\\n\")\n",
        "\n",
        "\n",
        "# --- Task 3: Adapt and Run Step 4 (Model Parallelism) ---\n",
        "\n",
        "print(\"--- Running Adapted Model Parallelism Example (Step 4) ---\")\n",
        "\n",
        "# Define devices\n",
        "device0 = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device1 = torch.device(\"cuda:1\" if torch.cuda.device_count() > 1 else device0)\n",
        "print(f\"Using device0: {device0}\")\n",
        "print(f\"Using device1: {device1} (Will be same as device0 if not multi-GPU)\")\n",
        "\n",
        "# Split model across two devices - ADAPTED\n",
        "class SplitNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SplitNet, self).__init__()\n",
        "        # 8 input features\n",
        "        self.part1 = nn.Linear(8, 256).to(device0)\n",
        "        # 2 output features\n",
        "        self.part2 = nn.Linear(256, 2).to(device1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device0)\n",
        "        x = F.relu(self.part1(x))\n",
        "        x = x.to(device1)\n",
        "        return self.part2(x)\n",
        "\n",
        "model_mp = SplitNet()\n",
        "# Criterion - ADAPTED for regression, and move to device1\n",
        "criterion_mp = nn.MSELoss().to(device1)\n",
        "\n",
        "# Optimizer needs parameters from both parts\n",
        "optimizer_mp = optim.Adam([\n",
        "    {'params': model_mp.part1.parameters()},\n",
        "    {'params': model_mp.part2.parameters()}\n",
        "], lr=0.001)\n",
        "\n",
        "# One-batch demonstration\n",
        "train_loader_mp = DataLoader(full_dataset, batch_size=64, shuffle=True)\n",
        "data, target = next(iter(train_loader_mp))\n",
        "\n",
        "# Move data to the correct starting devices\n",
        "data = data.to(device0)\n",
        "target = target.to(device1) # Target moves to where the loss is computed\n",
        "\n",
        "optimizer_mp.zero_grad()\n",
        "output = model_mp(data)\n",
        "loss = criterion_mp(output, target)\n",
        "loss.backward()\n",
        "optimizer_mp.step()\n",
        "\n",
        "print(\"Model parallelism step completed successfully.\")\n",
        "print(\"-\" * 30 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VmGdKP5maXc",
        "outputId": "9d8cdf05-dcca-4065-8c46-9d27a3155aaa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Describing ENB2012_data.csv ---\n",
            "Number of training samples: 768\n",
            "Features (8): ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']\n",
            "Target variables (2): ['Y1', 'Y2']\n",
            "This is a regression task, not a classification task.\n",
            "------------------------------\n",
            "\n",
            "--- Running Adapted Data Parallelism Example (Step 3) ---\n",
            "Using device: cpu\n",
            "Epoch 1 completed with averaged gradients.\n",
            "Epoch 2 completed with averaged gradients.\n",
            "------------------------------\n",
            "\n",
            "--- Running Adapted Model Parallelism Example (Step 4) ---\n",
            "Using device0: cpu\n",
            "Using device1: cpu (Will be same as device0 if not multi-GPU)\n",
            "Model parallelism step completed successfully.\n",
            "------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "15FYz9KRnuA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# --- Setup: Load Data and Define Device ---\n",
        "\n",
        "print(\"--- Loading and Describing ENB2012_data.csv ---\")\n",
        "# Load data\n",
        "file_name = 'ENB2012_data.csv'\n",
        "df = pd.read_csv(file_name)\n",
        "\n",
        "# Prepare data for PyTorch\n",
        "features = df.columns.drop(['Y1', 'Y2'])\n",
        "targets = ['Y1', 'Y2']\n",
        "X = torch.tensor(df[features].values.astype(np.float32))\n",
        "y = torch.tensor(df[targets].values.astype(np.float32))\n",
        "full_dataset = TensorDataset(X, y)\n",
        "full_loader = DataLoader(full_dataset, batch_size=64)\n",
        "\n",
        "# Define device\n",
        "# Note: On a single-GPU or CPU machine, all devices will be the same.\n",
        "# This is a *simulation* of parallelism.\n",
        "device_count = torch.cuda.device_count() if torch.cuda.is_available() else 0\n",
        "if device_count > 0:\n",
        "    device_list = [torch.device(f\"cuda:{i}\") for i in range(device_count)]\n",
        "else:\n",
        "    device_list = [torch.device(\"cpu\")] * 8 # Max devices we'll simulate\n",
        "\n",
        "device = device_list[0]\n",
        "print(f\"Running simulation on main device: {device}\")\n",
        "print(f\"Total available devices for simulation: {max(1, device_count)}\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Helper for Data Parallelism (DP)\n",
        "def compute_gradients_dp(data_loader, model, criterion, device):\n",
        "    model.zero_grad()\n",
        "    try:\n",
        "        data, target = next(iter(data_loader))\n",
        "    except StopIteration:\n",
        "        return None # Loader is empty\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    output = model(data)\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    grads = [p.grad.clone() for p in model.parameters() if p.grad is not None]\n",
        "    return grads\n",
        "\n",
        "# Helper to get final loss\n",
        "def get_final_loss(model, data_loader, criterion, device):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            # For MP models, data and target go to specific devices\n",
        "            if hasattr(model, 'part1'):\n",
        "                 data = data.to(model.part1.weight.device) # First device\n",
        "                 target = target.to(model.part_final.weight.device) # Last device\n",
        "            else:\n",
        "                 data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            total_loss += criterion(output, target).item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# --- Task 2.1: Data Parallelism Benchmark ---\n",
        "\n",
        "print(\"\\n--- Starting Data Parallelism Benchmark ---\")\n",
        "# Using MSE (Mean Squared Error) instead of \"accuracy\"\n",
        "dp_results = []\n",
        "splits_to_test = [2, 4, 8]\n",
        "N_EPOCHS = 5 # Number of simulated steps\n",
        "\n",
        "# Define the model architecture for DP\n",
        "class DPNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DPNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 128)\n",
        "        self.fc2 = nn.Linear(128, 2)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "for n_splits in splits_to_test:\n",
        "    print(f\"Testing DP with {n_splits} splits...\")\n",
        "    model_dp = DPNet().to(device)\n",
        "    optimizer_dp = optim.Adam(model_dp.parameters(), lr=0.001)\n",
        "    criterion_dp = nn.MSELoss()\n",
        "\n",
        "    # Create data splits\n",
        "    split_size = len(full_dataset) // n_splits\n",
        "    split_sizes = [split_size] * (n_splits - 1)\n",
        "    split_sizes.append(len(full_dataset) - sum(split_sizes))\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        datasets = random_split(full_dataset, split_sizes)\n",
        "        loaders = [DataLoader(ds, batch_size=64, shuffle=True) for ds in datasets]\n",
        "\n",
        "        all_grads = []\n",
        "        for loader in loaders:\n",
        "            grads = compute_gradients_dp(loader, model_dp, criterion_dp, device)\n",
        "            if grads:\n",
        "                all_grads.append(grads)\n",
        "\n",
        "        if not all_grads:\n",
        "            continue\n",
        "\n",
        "        # Average gradients\n",
        "        with torch.no_grad():\n",
        "            valid_params = [p for p in model_dp.parameters() if p.grad is not None]\n",
        "            for p_idx, p in enumerate(valid_params):\n",
        "                sum_grad = torch.stack([g[p_idx] for g in all_grads]).sum(dim=0)\n",
        "                p.grad = sum_grad / len(all_grads)\n",
        "\n",
        "        optimizer_dp.step()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    final_loss = get_final_loss(model_dp, full_loader, criterion_dp, device)\n",
        "    dp_results.append((n_splits, final_loss, training_time))\n",
        "\n",
        "print(\"--- DP Benchmark Complete ---\")\n",
        "\n",
        "# --- Task 2.2: Model Parallelism Benchmark ---\n",
        "\n",
        "print(\"\\n--- Starting Model Parallelism Benchmark ---\")\n",
        "mp_results = []\n",
        "parts_to_test = [2, 4]\n",
        "N_EPOCHS = 5 # Train for 5 epochs\n",
        "\n",
        "# Helper to get a device from our list, looping if needed\n",
        "def get_device(idx):\n",
        "    return device_list[idx % len(device_list)]\n",
        "\n",
        "# Define the MP model architectures\n",
        "class MPNet_2Parts(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MPNet_2Parts, self).__init__()\n",
        "        self.part1 = nn.Linear(8, 256).to(get_device(0))\n",
        "        self.part2 = nn.Linear(256, 2).to(get_device(1))\n",
        "        self.part_final = self.part2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.part1.weight.device)\n",
        "        x = F.relu(self.part1(x))\n",
        "        x = x.to(self.part2.weight.device)\n",
        "        x = self.part2(x)\n",
        "        return x\n",
        "\n",
        "class MPNet_4Parts(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MPNet_4Parts, self).__init__()\n",
        "        self.part1 = nn.Linear(8, 128).to(get_device(0))\n",
        "        self.part2 = nn.Linear(128, 256).to(get_device(1))\n",
        "        self.part3 = nn.Linear(256, 128).to(get_device(2))\n",
        "        self.part4 = nn.Linear(128, 2).to(get_device(3))\n",
        "        self.part_final = self.part4\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(self.part1.weight.device)\n",
        "        x = F.relu(self.part1(x))\n",
        "        x = x.to(self.part2.weight.device)\n",
        "        x = F.relu(self.part2(x))\n",
        "        x = x.to(self.part3.weight.device)\n",
        "        x = F.relu(self.part3(x))\n",
        "        x = x.to(self.part4.weight.device)\n",
        "        x = self.part4(x)\n",
        "        return x\n",
        "\n",
        "for n_parts in parts_to_test:\n",
        "    print(f\"Testing MP with {n_parts} parts...\")\n",
        "    if n_parts == 2:\n",
        "        model_mp = MPNet_2Parts()\n",
        "    else:\n",
        "        model_mp = MPNet_4Parts()\n",
        "\n",
        "    optimizer_mp = optim.Adam(model_mp.parameters(), lr=0.001)\n",
        "    final_device = model_mp.part_final.weight.device\n",
        "    criterion_mp = nn.MSELoss().to(final_device)\n",
        "\n",
        "    model_mp.train()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        for data, target in full_loader:\n",
        "            data = data.to(get_device(0)) # Data to first device\n",
        "            target = target.to(final_device) # Target to last device\n",
        "\n",
        "            optimizer_mp.zero_grad()\n",
        "            output = model_mp(data)\n",
        "            loss = criterion_mp(output, target)\n",
        "            loss.backward()\n",
        "            optimizer_mp.step()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    final_loss = get_final_loss(model_mp, full_loader, criterion_mp, device)\n",
        "    mp_results.append((n_parts, final_loss, training_time))\n",
        "\n",
        "print(\"--- MP Benchmark Complete ---\")\n",
        "\n",
        "# --- Task 2.3: Report Results ---\n",
        "\n",
        "print(\"\\n\\n--- Benchmark Results ---\")\n",
        "\n",
        "# Data Parallelism Table\n",
        "dp_df = pd.DataFrame(dp_results, columns=['Split Datasets', 'Final MSE', 'Training Time (s)'])\n",
        "print(\"\\nData Parallelism Results (Simulation):\")\n",
        "print(dp_df.to_string(index=False))\n",
        "\n",
        "# Model Parallelism Table\n",
        "mp_df = pd.DataFrame(mp_results, columns=['Model Parts', 'Final MSE', 'Training Time (s)'])\n",
        "print(\"\\nModel Parallelism Results (Simulation):\")\n",
        "print(mp_df.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdGfF9ejntfr",
        "outputId": "9d536bfa-42a4-450a-ee21-0918bda80992"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Loading and Describing ENB2012_data.csv ---\n",
            "Running simulation on main device: cpu\n",
            "Total available devices for simulation: 1\n",
            "\n",
            "--- Starting Data Parallelism Benchmark ---\n",
            "Testing DP with 2 splits...\n",
            "Testing DP with 4 splits...\n",
            "Testing DP with 8 splits...\n",
            "--- DP Benchmark Complete ---\n",
            "\n",
            "--- Starting Model Parallelism Benchmark ---\n",
            "Testing MP with 2 parts...\n",
            "Testing MP with 4 parts...\n",
            "--- MP Benchmark Complete ---\n",
            "\n",
            "\n",
            "--- Benchmark Results ---\n",
            "\n",
            "Data Parallelism Results (Simulation):\n",
            " Split Datasets   Final MSE  Training Time (s)\n",
            "              2  156.507947           0.020960\n",
            "              4  119.146825           0.035024\n",
            "              8 1712.233561           0.061678\n",
            "\n",
            "Model Parallelism Results (Simulation):\n",
            " Model Parts  Final MSE  Training Time (s)\n",
            "           2 103.379667           0.115690\n",
            "           4  40.459468           0.235382\n"
          ]
        }
      ]
    }
  ]
}