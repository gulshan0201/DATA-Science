{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsskCSaYpRncHmVEY5gUA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gulshan0201/DATA-Science/blob/main/Joblib_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNIe2pbiFJu1",
        "outputId": "3f3b51df-fe00-4a03-d1ab-e73ebfc22944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')  # Authorize once"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/colab_dbscan_demo'  # change if you like\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "\n",
        "ARTIFACT_PATH = os.path.join(BASE_DIR, 'dbscan_artifacts.joblib')\n",
        "print('Artifacts will be saved to:', ARTIFACT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K7GwjTSFdMT",
        "outputId": "12fd5a05-1ca1-4157-991e-c1083bfa4f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artifacts will be saved to: /content/drive/MyDrive/colab_dbscan_demo/dbscan_artifacts.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- TRAIN & SAVE (to Google Drive) ---\n",
        "import numpy as np\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "# Data: 3 blobs + 60 outliers\n",
        "X, _ = make_blobs(n_samples=1500, centers=[[1,1],[5,5],[9,1]],\n",
        "                  cluster_std=[0.35, 0.45, 0.4], random_state=42)\n",
        "rng = np.random.RandomState(42)\n",
        "outliers = rng.uniform(low=-2, high=12, size=(60, 2))\n",
        "X = np.vstack([X, outliers])\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# DBSCAN\n",
        "eps = 0.25\n",
        "min_samples = 8\n",
        "db = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)\n",
        "labels = db.fit_predict(X_scaled)\n",
        "\n",
        "# Core samples + NN index for inference\n",
        "core_mask = np.zeros_like(labels, dtype=bool)\n",
        "core_mask[db.core_sample_indices_] = True\n",
        "core_points = X_scaled[core_mask]\n",
        "core_labels = labels[core_mask]\n",
        "nn = NearestNeighbors(n_neighbors=5).fit(core_points)\n",
        "\n",
        "# Save all artifacts to Drive\n",
        "artifacts = {\n",
        "    \"scaler\": scaler,\n",
        "    \"dbscan\": db,\n",
        "    \"eps\": eps,\n",
        "    \"core_points\": core_points,\n",
        "    \"core_labels\": core_labels,\n",
        "    \"nn_index\": nn\n",
        "}\n",
        "joblib.dump(artifacts, ARTIFACT_PATH)\n",
        "\n",
        "# Summary\n",
        "unique, counts = np.unique(labels, return_counts=True)\n",
        "print(\"Cluster label counts (incl. noise=-1):\", dict(zip(unique.tolist(), counts.tolist())))\n",
        "print(\"Saved artifacts to:\", ARTIFACT_PATH)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFoY_sxVF99D",
        "outputId": "4540fbb3-4c68-4fcc-af54-362c34175888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster label counts (incl. noise=-1): {-1: 51, 0: 501, 1: 504, 2: 504}\n",
            "Saved artifacts to: /content/drive/MyDrive/colab_dbscan_demo/dbscan_artifacts.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob\n",
        "print('Listing artifacts in:', BASE_DIR)\n",
        "print(glob.glob(os.path.join(BASE_DIR, '*')))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvOjF3zLKaWH",
        "outputId": "2c1ab086-28f5-424f-b83c-995698c467ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listing artifacts in: /content/drive/MyDrive/colab_dbscan_demo\n",
            "['/content/drive/MyDrive/colab_dbscan_demo/dbscan_artifacts.joblib']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "def assign_label(new_points, artifact_path=ARTIFACT_PATH):\n",
        "    \"\"\"DBSCAN-style inference using saved core points in Drive.\"\"\"\n",
        "    art = joblib.load(artifact_path)\n",
        "    scaler = art[\"scaler\"]\n",
        "    eps = art[\"eps\"]\n",
        "    nn = art[\"nn_index\"]\n",
        "    core_points = art[\"core_points\"]\n",
        "    core_labels = art[\"core_labels\"]\n",
        "\n",
        "    Z = scaler.transform(np.asarray(new_points))\n",
        "    inds_list = nn.radius_neighbors(Z, radius=eps, return_distance=False)\n",
        "\n",
        "    out = []\n",
        "    for inds in inds_list:\n",
        "        if len(inds) == 0:\n",
        "            out.append(-1)  # anomaly\n",
        "        else:\n",
        "            lbls = core_labels[inds]\n",
        "            vals, cnts = np.unique(lbls, return_counts=True)\n",
        "            out.append(int(vals[np.argmax(cnts)]))\n",
        "    return np.array(out)\n",
        "\n",
        "# Try a few points\n",
        "new_samples = np.array([\n",
        "    [1.1, 0.9],\n",
        "    [4.9, 5.2],\n",
        "    [9.2, 1.1],\n",
        "    [15.0, -3.0]  # likely anomaly\n",
        "])\n",
        "print(\"Assigned labels (-1 = anomaly):\", assign_label(new_samples))\n"
      ],
      "metadata": {
        "id": "WQHJqgiqKl6f",
        "outputId": "82b2022b-b27b-446d-ce5b-10f126469323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assigned labels (-1 = anomaly): [ 1  0  2 -1]\n"
          ]
        }
      ]
    }
  ]
}